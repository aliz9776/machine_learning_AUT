{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwvxJGsda8Aq"
      },
      "source": [
        "**Problem 3: Sentiment Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuU-0DaebG3q"
      },
      "source": [
        "Snappfood (an online food delivery company) user comments containing 70,000 comments with two labels: 1- Happy (Positive) 2- Sad (Negative). We want you to build naive bayes Classifier from scratch to perform sentiment analysis. (This link1 can be very useful). Follow the steps below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oHBy3zea7NJ",
        "outputId": "a422db33-e0ad-4add-b34f-34b756aed7ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: hazm in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: libwapiti>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from hazm) (0.2.1)\n",
            "Requirement already satisfied: nltk==3.3 in /usr/local/lib/python3.10/dist-packages (from hazm) (3.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from nltk==3.3->hazm) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install hazm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbdY4XWbcSh4"
      },
      "source": [
        "a) Before building the model, you must do the required text preprocessing. Explain all pre-processing steps. (You can use the hazm library, which is used to process the Persian language. (Note that this section has the highest score)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "taJwoKIadJFd"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "from hazm import *\n",
        "\n",
        "import re\n",
        "\n",
        "\n",
        "normalizer = Normalizer()\n",
        "lemmatizer = Lemmatizer()\n",
        "tokenizer = WordTokenizer()\n",
        "\n",
        "def preprocess(text):\n",
        "    \n",
        "    # Remove all punctuations except for dot and question mark\n",
        "    text = re.sub(r'[^\\w\\s؟.]', '', text)\n",
        "    \n",
        "    # Normalize text\n",
        "    text = normalizer.normalize(text)\n",
        "    \n",
        "    # Tokenize text\n",
        "    words = tokenizer.tokenize(text)\n",
        "    \n",
        "    # Lemmatize words and remove stop words\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if not word in stopwords_list()]\n",
        "    \n",
        "    # Join the words back into a string\n",
        "    text = ' '.join(words)\n",
        "    \n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKoK4skEmeUe"
      },
      "outputs": [],
      "source": [
        "import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PX3_HZSscQE3",
        "outputId": "35d01b7c-974e-41c8-8eac-19cad0313360"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0      واقعا حیف وقت که بنویسم سرویس دهیتون شده افتضاح\n",
            "1    قرار بود ۱ ساعته برسه ولی نیم ساعت زودتر از مو...\n",
            "2    قیمت این مدل اصلا با کیفیتش سازگاری نداره، فقط...\n",
            "3    عالللی بود همه چه درست و به اندازه و کیفیت خوب...\n",
            "4                        شیرینی وانیلی فقط یک مدل بود.\n",
            "Name: comment, dtype: object\n",
            "0      واقعا حیف وقت که بنویسم سرویس دهیتون شده افتضاح\n",
            "1    قرار بود ۱ ساعته برسه ولی نیم ساعت زودتر از مو...\n",
            "2    قیمت این مدل اصلا با کیفیتش سازگاری نداره، فقط...\n",
            "3    عالللی بود همه چه درست و به اندازه و کیفیت خوب...\n",
            "4                        شیرینی وانیلی فقط یک مدل بود.\n",
            "Name: comment, dtype: object\n",
            "0    [واقعا, حیف, وقت, که, بنویسم, سرویس, دهیتون, ش...\n",
            "1    [قرار, بود, ۱, ساعته, برسه, ولی, نیم, ساعت, زو...\n",
            "2    [قیمت, این, مدل, اصلا, با, کیفیتش, سازگاری, ند...\n",
            "3    [عالللی, بود, همه, چه, درست, و, به, اندازه, و,...\n",
            "4               [شیرینی, وانیلی, فقط, یک, مدل, بود, .]\n",
            "Name: comment, dtype: object\n",
            "0     [واقعا, حیف, وقت, بنویسم, سرویس, دهیتون, افتضاح]\n",
            "1    [قرار, ۱, ساعته, برسه, نیم, ساعت, زودتر, موقع,...\n",
            "2    [قیمت, مدل, اصلا, کیفیتش, سازگاری, نداره, ،, ظ...\n",
            "3    [عالللی, درست, اندازه, کیفیت, ،, امیداورم, کیف...\n",
            "4                             [شیرینی, وانیلی, مدل, .]\n",
            "Name: comment, dtype: object\n",
            "0       [واقعا, حیف, وق, بنویس, سرویس, دهیتون, افتضاح]\n",
            "1    [قرار, ۱, ساعته, برسه, ن, ساع, زود, موقع, ،, ب...\n",
            "2    [قیم, مدل, اصلا, کیفیت, سازگار, نداره, ،, ظاهر...\n",
            "3    [عاللل, درس, اندازه, کیف, ،, امیداور, کیفیتتون...\n",
            "4                               [شیرین, وانیل, مدل, .]\n",
            "5                               [بد, پیتزا, خورده_بود]\n",
            "6                                              [ممنون]\n",
            "7    [کیف, غذا, متوسط, پایین, انگار, داخل, یه, رستو...\n",
            "8    [اقلا, تازه, روز, وخیلییی, سریع, بدس, واقعا, م...\n",
            "9     [چ, ه, داگ, دور, کلا, سوخته_بود, داخل, خا, !!!!]\n",
            "Name: comment, dtype: object\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from hazm import word_tokenize\n",
        "from hazm import Normalizer\n",
        "from hazm import stopwords_list\n",
        "from hazm import Stemmer\n",
        "\n",
        "df = pd.read_csv('Snappfood - Sentiment Analysis.csv', delimiter='\\t')\n",
        "\n",
        "#Remove unnecessary column\n",
        "\n",
        "df = df.drop('label_id', axis=1)\n",
        "\n",
        "# remove the first column which is empty\n",
        "df.drop(df.columns[0], axis=1, inplace=True)\n",
        "\n",
        "#Convert the labels to numerical values:\n",
        "\n",
        "label_to_id = {'HAPPY': 0, 'SAD': 1}\n",
        "df['label'] = df['label'].map(label_to_id)\n",
        "\n",
        "## Normalize the tokens using the hazm library\n",
        "\n",
        "# Apply the normalization function to the comments column\n",
        "normalizer = Normalizer()\n",
        "df['comment'] = df['comment'].apply(normalizer.normalize)\n",
        "print(df['comment'].head())\n",
        "\n",
        "#Tokenize the comments using the hazm library\n",
        "print(df['comment'].head())\n",
        "df['comment'] = df['comment'].apply(word_tokenize)\n",
        "print(df['comment'].head())\n",
        "\n",
        "#Remove stop words using the hazm library\n",
        "\n",
        "stopwords = stopwords_list()\n",
        "df['comment'] = df['comment'].apply(lambda tokens: [token for token in tokens if token not in stopwords])\n",
        "print(df['comment'].head())\n",
        "#Stem the tokens using the hazm library\n",
        "\n",
        "stemmer = Stemmer()\n",
        "df['comment'] = df['comment'].apply(lambda tokens: [stemmer.stem(token) for token in tokens])\n",
        "\n",
        "print(df['comment'].head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sF9Zf_HevrDR"
      },
      "source": [
        "b) Building the naive bayes classifier. Explain how naive bayes is used for this problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xdPB9Q6vrKx",
        "outputId": "fb45f2b5-c54c-42ad-9bda-9569d6f47eb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0               واقعا حیف وق بنویس سرویس دهیتون افتضاح\n",
            "1    قرار ۱ ساعته برسه ن ساع زود موقع ، ببین چقدررر...\n",
            "2    قیم مدل اصلا کیفیت سازگار نداره ، ظاهر فریبنده...\n",
            "3    عاللل درس اندازه کیف ، امیداور کیفیتتون باشه م...\n",
            "4                                    شیرین وانیل مدل .\n",
            "5                                   بد پیتزا خورده_بود\n",
            "6                                                ممنون\n",
            "7    کیف غذا متوسط پایین انگار داخل یه رستور معمول ...\n",
            "8           اقلا تازه روز وخیلییی سریع بدس واقعا متشکر\n",
            "9               چ ه داگ دور کلا سوخته_بود داخل خا !!!!\n",
            "Name: comment, dtype: object\n",
            "comment      0\n",
            "label      520\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Convert the list of tokenized words in 'comment' column to strings\n",
        "df['comment'] = df['comment'].apply(lambda x: ' '.join(x))\n",
        "print(df['comment'].head(10))\n",
        "print(df.isnull().sum())\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Convert preprocessed comments to numerical representation using TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(df['comment'])\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "train_size = int(len(df) * 0.8)\n",
        "X_train, X_test = X[:train_size], X[train_size:]\n",
        "y_train, y_test = df['label'][:train_size], df['label'][train_size:]\n",
        "\n",
        "# Train the Naive Bayes classifier\n",
        "clf = MultinomialNB()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5FSpM7JXSuT"
      },
      "source": [
        "c) Fitting the model on training set and evaluating accuracies on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BcnIJo1XS3J",
        "outputId": "225e4b75-14cf-4c81-97c7-25cfd1aa4b18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.80\n",
            "Confusion Matrix:\n",
            "[[5080 1921]\n",
            " [ 839 6056]]\n",
            "Precision: 0.8090957260141125\n",
            "Recall: 0.8013816925734024\n",
            "F1-score: 0.8002900902166333\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support\n",
        "\n",
        "\n",
        "\n",
        "#fit classifier\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Test the classifier\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Calculate precision, recall, and F1-score\n",
        "precision, recall, f1, support = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
        "\n",
        "# Print results\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-score:\", f1)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
